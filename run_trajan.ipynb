{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import dataclasses\n",
    "import einops\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tapnet.trajan import track_autoencoder\n",
    "from tapnet.tapvid import evaluation_datasets\n",
    "from tapnet.utils import transforms\n",
    "from tapnet.utils import viz_utils\n",
    "from tapnet.utils import model_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npload(fname):\n",
    "  loaded = np.load(fname, allow_pickle=False)\n",
    "  if isinstance(loaded, np.ndarray):\n",
    "    return loaded\n",
    "  else:\n",
    "    return dict(loaded)\n",
    "\n",
    "\n",
    "def recover_tree(flat_dict):\n",
    "  tree = (\n",
    "      {}\n",
    "  )  # Initialize an empty dictionary to store the resulting tree structure\n",
    "  for (\n",
    "      k,\n",
    "      v,\n",
    "  ) in (\n",
    "      flat_dict.items()\n",
    "  ):  # Iterate over each key-value pair in the flat dictionary\n",
    "    parts = k.split(\n",
    "        '/'\n",
    "    )  # Split the key into parts using \"/\" as a delimiter to build the tree structure\n",
    "    node = tree  # Start at the root of the tree\n",
    "    for part in parts[\n",
    "        :-1\n",
    "    ]:  # Loop through each part of the key, except the last one\n",
    "      if (\n",
    "          part not in node\n",
    "      ):  # If the current part doesn't exist as a key in the node, create an empty dictionary for it\n",
    "        node[part] = {}\n",
    "      node = node[part]  # Move down the tree to the next level\n",
    "    node[parts[-1]] = v  # Set the value at the final part of the key\n",
    "  return tree  # Return the reconstructed tree\n",
    "\n",
    "trajan_checkpoint_path = \"/restricted/projectnb/cs599dg/mwakeham/tapnet/checkpoints/track_autoencoder_ckpt.npz\"\n",
    "params = recover_tree(npload(trajan_checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Preprocessor for Tracks\n",
    "\n",
    "@dataclasses.dataclass(kw_only=True, frozen=True, eq=True)\n",
    "class ProcessTracksForTrackAutoencoder:\n",
    "  \"\"\"Samples tracks and fills out support_tracks, query_points etc.\n",
    "\n",
    "  TrackAutoencoder format which will be output from this transform:\n",
    "   video: float[\"*B T H W 3\"]\n",
    "   support_tracks: float[\"*B QS T 2\"]\n",
    "   support_tracks_visible: float[\"*B QS T 1\"]\n",
    "   query_points: float[\"*B Q 3\"]\n",
    "  \"\"\"\n",
    "\n",
    "  # note that we do not use query points in the encoding, so it is expected\n",
    "  # that num_support_tracks >> num_target_tracks\n",
    "\n",
    "  num_support_tracks: int\n",
    "  num_target_tracks: int\n",
    "\n",
    "  # If true, assume that everything after the boundary_frame is padding,\n",
    "  # so don't sample any query points after the boundary_frame, and only sample\n",
    "  # target tracks that have at least one visible frame before the boundary.\n",
    "  before_boundary: bool = True\n",
    "  episode_length: int = 150\n",
    "\n",
    "  # Keys.\n",
    "  video_key: str = \"video\"\n",
    "  tracks: str = \"tracks\"  # [time, num_points, 2]\n",
    "  visible_key: str = \"visible\"  # [time, num_points, 1]\n",
    "\n",
    "  def random_map(self, features):\n",
    "\n",
    "    # set tracks xy and compute visibility\n",
    "    tracks_xy = features[self.tracks][..., :2]\n",
    "    tracks_xy = np.asarray(tracks_xy, np.float32)\n",
    "    boundary_frame = features[\"video\"].shape[0]\n",
    "\n",
    "    # visibles already post-processed by compute_point_tracks.py\n",
    "    visibles = np.asarray(features[self.visible_key], np.float32)\n",
    "\n",
    "    # pad to 'episode_length' frames\n",
    "    if self.before_boundary:\n",
    "      # if input video is longer than episode_length, crop to episode_length\n",
    "      if self.episode_length - visibles.shape[0] < 0:\n",
    "        visibles = visibles[: self.episode_length]\n",
    "        tracks_xy = tracks_xy[: self.episode_length]\n",
    "\n",
    "      visibles = np.pad(\n",
    "          visibles,\n",
    "          [[0, self.episode_length - visibles.shape[0]], [0, 0]],\n",
    "          constant_values=0.0,\n",
    "      )\n",
    "      tracks_xy = np.pad(\n",
    "          tracks_xy,\n",
    "          [[0, self.episode_length - tracks_xy.shape[0]], [0, 0], [0, 0]],\n",
    "          constant_values=0.0,\n",
    "      )\n",
    "\n",
    "    # Samples indices for support tracks and target tracks.\n",
    "    num_input_tracks = tracks_xy.shape[1]\n",
    "    idx = np.arange(num_input_tracks)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    assert (\n",
    "        num_input_tracks >= self.num_support_tracks + self.num_target_tracks\n",
    "    ), (\n",
    "        (\n",
    "            f\"num_input_tracks {num_input_tracks} must be greater than\"\n",
    "            f\" num_support_tracks {self.num_support_tracks} + num_target_tracks\"\n",
    "            f\" {self.num_target_tracks}\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    idx_support = idx[-self.num_support_tracks :]\n",
    "    idx_target = idx[: self.num_target_tracks]\n",
    "\n",
    "    # Gathers support tracks from `features`.  Features are of shape\n",
    "    # [time, num_points, 2]\n",
    "    support_tracks = tracks_xy[..., idx_support, :]\n",
    "    support_tracks_visible = visibles[..., idx_support]\n",
    "\n",
    "    # Gathers target tracks from `features`.\n",
    "    target_tracks = tracks_xy[..., idx_target, :]\n",
    "    target_tracks_visible = visibles[..., idx_target]\n",
    "\n",
    "    # transpose to [num_points, time, ...]\n",
    "    support_tracks = np.transpose(support_tracks, [1, 0, 2])\n",
    "    support_tracks_visible = np.expand_dims(\n",
    "        np.transpose(support_tracks_visible, [1, 0]), axis=-1\n",
    "    )\n",
    "\n",
    "    # [time, point_id, x/y] -> [point_id, time, x/y]\n",
    "    target_tracks = np.transpose(target_tracks, [1, 0, 2])\n",
    "    target_tracks_visible = np.transpose(target_tracks_visible, [1, 0])\n",
    "\n",
    "    # Sample query points as random visible points\n",
    "    num_target_tracks = target_tracks_visible.shape[0]\n",
    "    num_frames = target_tracks_visible.shape[1]\n",
    "    random_frame = np.zeros(num_target_tracks, dtype=np.int64)\n",
    "\n",
    "    for i in range(num_target_tracks):\n",
    "      visible_indices = np.where(target_tracks_visible[i] > 0)[0]\n",
    "      if len(visible_indices) > 0:\n",
    "        # Choose a random frame index from the visible ones\n",
    "        random_frame[i] = np.random.choice(visible_indices)\n",
    "      else:\n",
    "        # If no frame is visible for a track, default to frame 0\n",
    "        # (or handle as appropriate for your use case)\n",
    "        random_frame[i] = 0\n",
    "\n",
    "    # Create one-hot encoding based on the randomly selected frame for each track\n",
    "    idx = np.eye(num_frames, dtype=np.float32)[\n",
    "        random_frame\n",
    "    ]  # [num_target_tracks, num_frames]\n",
    "\n",
    "    # Use the one-hot index to select the coordinates at the chosen frame\n",
    "    target_queries_xy = np.sum(\n",
    "        target_tracks * idx[..., np.newaxis], axis=1\n",
    "    )  # [num_target_tracks, 2]\n",
    "\n",
    "    # Stack frame index and coordinates: [t, x, y]\n",
    "    target_queries = np.stack(\n",
    "        [\n",
    "            random_frame.astype(np.float32),\n",
    "            target_queries_xy[..., 0],\n",
    "            target_queries_xy[..., 1],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )  # [num_target_tracks, 3]\n",
    "\n",
    "    # Add channel dimension to target_tracks_visible\n",
    "    target_tracks_visible = np.expand_dims(target_tracks_visible, axis=-1)\n",
    "\n",
    "    # Updates `features` to contain these *new* features and add batch dim.\n",
    "    features_new = {\n",
    "        \"support_tracks\": support_tracks[None, :],\n",
    "        \"support_tracks_visible\": support_tracks_visible[None, :],\n",
    "        \"query_points\": target_queries[None, :],\n",
    "        \"target_points\": target_tracks[None, :],\n",
    "        \"boundary_frame\": np.array([boundary_frame]),\n",
    "        \"target_tracks_visible\": target_tracks_visible[None, :],\n",
    "        \"target_tracks_indices\": idx_target[None, :]\n",
    "    }\n",
    "    features.update(features_new)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model and preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "preprocessor = ProcessTracksForTrackAutoencoder(\n",
    "    num_support_tracks=64,\n",
    "    num_target_tracks=32,\n",
    "    video_key=\"video\", \n",
    "    before_boundary=True,\n",
    ")\n",
    "\n",
    "trajan_checkpoint_path = \"/restricted/projectnb/cs599dg/mwakeham/tapnet/checkpoints/track_autoencoder_ckpt.npz\"\n",
    "params = recover_tree(npload(trajan_checkpoint_path))\n",
    "model = track_autoencoder.TrackAutoEncoder(decoder_scan_chunk_size=32)\n",
    "\n",
    "@jax.jit\n",
    "def forward(params, inputs):\n",
    "    outputs, latents = model.apply({'params': params}, inputs)\n",
    "    return outputs, latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load existing tracks and run trajan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = \"tennis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tracks(tracks, width=640, height=360):\n",
    "    tracks_normalized = tracks.copy()\n",
    "    tracks_normalized[..., 0] = tracks[..., 0] / width\n",
    "    tracks_normalized[..., 1] = tracks[..., 1] / height\n",
    "    return tracks_normalized\n",
    "\n",
    "data_dir = f\"/restricted/projectnb/cs599dg/mwakeham/trajectory_examples/{scene}\"\n",
    "\n",
    "example_dirs = [d for d in os.listdir(data_dir) if d.startswith(\"example\")]\n",
    "example_dirs.sort()\n",
    "\n",
    "all_outputs = {}\n",
    "batch_data = {}\n",
    "all_latents = {}\n",
    "\n",
    "for example_dir in tqdm(example_dirs):\n",
    "    full_path = os.path.join(data_dir, example_dir)\n",
    "    \n",
    "    # Process each view in this example\n",
    "    while True:\n",
    "        tracks_path = os.path.join(full_path, f\"tracks_view{view_idx}.npy\")\n",
    "        visible_path = os.path.join(full_path, f\"visible_view{view_idx}.npy\")\n",
    "        \n",
    "        if not os.path.exists(tracks_path):\n",
    "            break\n",
    "            \n",
    "        tracks = np.load(tracks_path)\n",
    "        visible = np.load(visible_path)\n",
    "        \n",
    "        tracks = normalize_tracks(tracks, width=640, height=360)\n",
    "        \n",
    "        if len(visible.shape) == 3 and visible.shape[2] == 1:\n",
    "            visible = visible.squeeze(-1)\n",
    "        \n",
    "        dummy_video = np.zeros((150, 360, 640, 3), dtype=np.float32)\n",
    "        \n",
    "        batch = {\n",
    "            \"video\": dummy_video,\n",
    "            \"tracks\": tracks.astype(np.float32),\n",
    "            \"visible\": visible.astype(np.float32),\n",
    "        }\n",
    "        \n",
    "        batch = preprocessor.random_map(batch)\n",
    "        batch.pop(\"tracks\", None)\n",
    "        outputs, latents = forward(params, batch)\n",
    "        \n",
    "        if example_dir not in all_outputs:\n",
    "            all_outputs[example_dir] = {}\n",
    "            batch_data[example_dir] = {}\n",
    "            all_latents[example_dir] = {}\n",
    "            \n",
    "        all_outputs[example_dir][f'view{view_idx}'] = outputs\n",
    "        batch_data[example_dir][f'view{view_idx}'] = batch\n",
    "        all_latents[example_dir][f'view{view_idx}'] = latents\n",
    "        \n",
    "        view_idx += 1\n",
    "\n",
    "print(f\"Processed {len(all_outputs)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = {}\n",
    "jaccard_scores = []\n",
    "occlusion_scores = []\n",
    "\n",
    "for example_name, views_data in tqdm(all_outputs.items()):\n",
    "    all_metrics[example_name] = {}\n",
    "    \n",
    "    for view_name, outputs in views_data.items():\n",
    "        batch = batch_data[example_name][view_name]\n",
    "        \n",
    "        height, width = 360, 640\n",
    "        \n",
    "        reconstructed_tracks = transforms.convert_grid_coordinates(\n",
    "            outputs.tracks[0], (1, 1), (width, height)\n",
    "        )\n",
    "\n",
    "        target_tracks_vis = transforms.convert_grid_coordinates(\n",
    "            batch['target_points'][0], (1, 1), (width, height)\n",
    "        )\n",
    "\n",
    "        reconstructed_visibles = model_utils.postprocess_occlusions(\n",
    "            outputs.visible_logits, outputs.certain_logits\n",
    "        )\n",
    "\n",
    "        video_length = batch['boundary_frame'][0]\n",
    "\n",
    "        query_points = np.zeros((\n",
    "            reconstructed_visibles.shape[0],\n",
    "            batch['target_tracks_visible'].shape[1],\n",
    "            1,\n",
    "        ))\n",
    "\n",
    "        # Compute TapVid metrics\n",
    "        metrics = evaluation_datasets.compute_tapvid_metrics(\n",
    "            query_points=query_points,\n",
    "            gt_occluded=1 - batch['target_tracks_visible'][..., :video_length, 0],\n",
    "            gt_tracks=target_tracks_vis[None, ..., :video_length, :],\n",
    "            pred_occluded=reconstructed_visibles[..., :video_length, 0],\n",
    "            pred_tracks=reconstructed_tracks[..., :video_length, :],\n",
    "            query_mode='strided',\n",
    "            get_trackwise_metrics=False,\n",
    "        )\n",
    "\n",
    "        jaccard = np.mean([metrics[f'jaccard_{d}'] for d in [1, 2, 4, 8, 16]])\n",
    "        occlusion_acc = metrics['occlusion_accuracy'].mean()\n",
    "        \n",
    "        jaccard_scores.append(jaccard)\n",
    "        occlusion_scores.append(occlusion_acc)\n",
    "        \n",
    "        all_metrics[example_name][view_name] = {\n",
    "            'average_jaccard': jaccard,\n",
    "            'occlusion_accuracy': occlusion_acc,\n",
    "            'full_metrics': metrics\n",
    "        }\n",
    "\n",
    "final_avg_jaccard = np.mean(jaccard_scores)\n",
    "final_avg_occlusion = np.mean(occlusion_scores)\n",
    "\n",
    "print(f\"Final Average Jaccard: {final_avg_jaccard:.4f}\")\n",
    "print(f\"Final Average Occlusion Accuracy: {final_avg_occlusion:.4f}\")\n",
    "print(f\"Processed {len(jaccard_scores)} total views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = f\"/restricted/projectnb/cs599dg/mwakeham/trajectory_examples_trajan_results/{scene}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving metrics to: {results_dir}\")\n",
    "\n",
    "for example_name, views_data in tqdm(all_outputs.items()):\n",
    "    example_dir = os.path.join(results_dir, example_name)\n",
    "    os.makedirs(example_dir, exist_ok=True)\n",
    "    \n",
    "    for view_name, outputs in views_data.items():\n",
    "        output_path = os.path.join(example_dir, f\"trajan_metrics_view{view_name.replace('view', '')}.npz\")\n",
    "        \n",
    "        metrics = all_metrics[example_name][view_name]\n",
    "        \n",
    "        np.savez(\n",
    "            output_path,\n",
    "            average_jaccard=metrics['average_jaccard'],\n",
    "            occlusion_accuracy=metrics['occlusion_accuracy'],\n",
    "            full_metrics=metrics['full_metrics']\n",
    "        )\n",
    "\n",
    "        latents_path = os.path.join(example_dir, f\"trajan_latents_view{view_name.replace('view', '')}.npz\")\n",
    "        latents = all_latents[example_name][view_name]\n",
    "        \n",
    "        np.savez(\n",
    "            latents_path,\n",
    "            encoded_latents=latents\n",
    "        )\n",
    "\n",
    "summary = {\n",
    "    'final_avg_jaccard': final_avg_jaccard,\n",
    "    'final_avg_occlusion': final_avg_occlusion,\n",
    "    'total_views_processed': len(jaccard_scores),\n",
    "    'all_jaccard_scores': jaccard_scores,\n",
    "    'all_occlusion_scores': occlusion_scores,\n",
    "    'scene': scene\n",
    "}\n",
    "\n",
    "np.save(os.path.join(results_dir, \"overall_summary.npy\"), summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
